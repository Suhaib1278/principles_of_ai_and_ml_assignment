{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set the maximum number of rows to display to a large number\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Fetch dataset\n",
    "data = fetch_ucirepo(id=697)\n",
    "\n",
    "# Create a single DataFrame during import\n",
    "df = data.data.original\n",
    "\n",
    "# Check the structure of the DataFrame\n",
    "df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4424 entries and 4424 non-null values in each column \n",
    "# But just to verify\n",
    "df.isnull().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset and information about it can be found here: https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success \n",
    "\n",
    "The following columns are categorical (some represented numerically):\n",
    "* Marital status\n",
    "* Application mode\n",
    "* Course\n",
    "* Daytime/evening attendance\n",
    "* Previous qualification\n",
    "* Nacionality\n",
    "* Mother's qualification\n",
    "* Father's qualification\n",
    "* Mother's occupation\n",
    "* Father's occupation\n",
    "* Displaced\n",
    "* Educational special needs\n",
    "* Debtor\n",
    "* Tuition fees up to date\n",
    "* Gender\n",
    "* Scholarship holder\n",
    "* International\n",
    "* Target\n",
    "\n",
    "The following columns are numerical (discrete):\n",
    "* Application order\n",
    "* Age at enrollment\n",
    "* Curricular units 1st sem (credited)             \n",
    "* Curricular units 1st sem (enrolled)\n",
    "* Curricular units 1st sem (evaluations)\n",
    "* Curricular units 1st sem (approved)\n",
    "* Curricular units 1st sem (without evaluations)\n",
    "* Curricular units 2nd sem (credited)\n",
    "* Curricular units 2nd sem (enrolled)\n",
    "* Curricular units 2nd sem (evaluations)\n",
    "* Curricular units 2nd sem (approved)\n",
    "* Curricular units 2nd sem (without evaluations)\n",
    "\n",
    "The following columns are numerical (continuous):\n",
    "* Previous qualification (grade)\n",
    "* Admission grade\n",
    "* Curricular units 1st sem (grade)\n",
    "* Curricular units 2nd sem (grade)\n",
    "* Unemployment rate\n",
    "* Inflation rate\n",
    "* GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of 0s in each column\n",
    "for col in df.columns:\n",
    "    zeros_in_col = (df[col] == 0).sum()\n",
    "    print(f\"Zeros in {col} column: {zeros_in_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values in each column\n",
    "# cols_not_to_check_unique_vals_in = [\"Height\", \"Weight\", ]\n",
    "for col in df:\n",
    "# if col not in cols_not_to_check_unique_vals_in:\n",
    "        # print(col)\n",
    "        print(f\"{col} column no. unique values: {df[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous columns - Previous qualification (grade), Admission grade, Unemployment rate, Inflation rate, GDP\n",
    "# Can be found here - https://storage.googleapis.com/kaggle-forum-message-attachments/1832313/17922/Features%20information.pdf\n",
    "# Check the unique values in the discrete columns\n",
    "cols_not_to_check_unique_vals_in = [\"Previous qualification (grade)\", \"Admission grade\", \"Curricular units 1st sem (grade)\", \"Curricular units 2nd sem (grade)\"]\n",
    "for col in df.columns:\n",
    "    if col not in cols_not_to_check_unique_vals_in:\n",
    "        print(f\"Unique values in {col} column: {df[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some plotting next to understand the data\n",
    "\n",
    "categorical_cols = [\n",
    "    \"Marital Status\",\n",
    "    \"Application mode\",\n",
    "    \"Course\",\n",
    "    \"Daytime/evening attendance\",\n",
    "    \"Previous qualification\",\n",
    "    \"Nacionality\",\n",
    "    \"Mother's qualification\",\n",
    "    \"Father's qualification\",\n",
    "    \"Mother's occupation\",\n",
    "    \"Father's occupation\",\n",
    "    \"Displaced\",\n",
    "    \"Educational special needs\",\n",
    "    \"Debtor\",\n",
    "    \"Tuition fees up to date\",\n",
    "    \"Gender\",\n",
    "    \"Scholarship holder\",\n",
    "    \"International\",\n",
    "    \"Target\"\n",
    "]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(col)\n",
    "    value_counts = df[col].value_counts()\n",
    "    value_counts.plot(kind='bar')\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xlabel(col)\n",
    "    plt.grid()\n",
    "    plt.title(f\"{col} count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = [\n",
    "    \"Application order\",\n",
    "    \"Age at enrollment\",\n",
    "    \"Curricular units 1st sem (credited)\",\n",
    "    \"Curricular units 1st sem (enrolled)\",\n",
    "    \"Curricular units 1st sem (evaluations)\",\n",
    "    \"Curricular units 1st sem (approved)\",\n",
    "    \"Curricular units 1st sem (without evaluations)\",\n",
    "    \"Curricular units 2nd sem (credited)\",\n",
    "    \"Curricular units 2nd sem (enrolled)\",\n",
    "    \"Curricular units 2nd sem (evaluations)\",\n",
    "    \"Curricular units 2nd sem (approved)\",\n",
    "    \"Curricular units 2nd sem (without evaluations)\",\n",
    "    \"Previous qualification (grade)\",\n",
    "    \"Admission grade\",\n",
    "    \"Curricular units 1st sem (grade)\",\n",
    "    \"Curricular units 2nd sem (grade)\",\n",
    "    \"Unemployment rate\",\n",
    "    \"Inflation rate\",\n",
    "    \"GDP\"\n",
    "]\n",
    "\n",
    "# for col in numerical_cols:\n",
    "#     print(col)\n",
    "#     value_counts = df[col].value_counts()\n",
    "#     value_counts.plot(kind='hist')\n",
    "#     plt.ylabel(\"Count\")\n",
    "#     plt.xlabel(col)\n",
    "#     plt.show()\n",
    "\n",
    "# for col in numerical_cols:\n",
    "#     print(col)\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "\n",
    "#     # Plot histogram with horizontal orientation\n",
    "#     plt.hist(df[col], orientation='vertical', color='skyblue', edgecolor='black')\n",
    "\n",
    "#     plt.xlabel(\"Count\")         # Count on the x-axis\n",
    "#     plt.ylabel(col)             # Column values on the y-axis\n",
    "#     plt.title(f\"Histogram of {col}\")\n",
    "#     plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "#     plt.show()\n",
    "\n",
    "# for col in numerical_cols:\n",
    "#     print(col)\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "    \n",
    "#     # Plot boxplot\n",
    "#     plt.boxplot(df[col], vert=True, patch_artist=True, boxprops=dict(facecolor='skyblue'))\n",
    "    \n",
    "#     plt.ylabel(col)             # Column values on the y-axis\n",
    "#     plt.title(f\"Boxplot of {col}\")\n",
    "#     plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "#     plt.show()\n",
    "\n",
    "for col in numerical_cols:\n",
    "    print(col)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Plot boxplot\n",
    "    plt.boxplot(df[col], vert=True, patch_artist=True, boxprops=dict(facecolor='skyblue'))\n",
    "    \n",
    "    plt.ylabel(col)             # Column values on the y-axis\n",
    "    plt.title(f\"Boxplot of {col}\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # Remove or customize the x-axis tick label\n",
    "    plt.xticks([1], [''])  # This removes the label or you can replace '' with a custom label\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers(col):\n",
    "    Q1 = col.quantile(0.25)\n",
    "    Q3 = col.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    # print(\"IQR used\")\n",
    "    outlier_mask = (col < Q1 - 1.5 * IQR) | (col > Q3 + 1.5 * IQR)\n",
    "    outlier_values = col[outlier_mask].unique().tolist()  # Remove duplicates and convert to list\n",
    "    return outlier_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_cols = []\n",
    "for col in numerical_cols:\n",
    "    outliers = handle_outliers(df[col])\n",
    "    if outliers == []:\n",
    "        print(f\"No outliers in {col} column\\n\")\n",
    "    else:\n",
    "        outlier_cols.append(col)\n",
    "        print(f\"Outliers in {col} column:\")\n",
    "        print(f\"{outliers}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in outlier_cols:\n",
    "    print(col)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Plot histogram with horizontal orientation\n",
    "    plt.hist(df[col], orientation='vertical', color='skyblue', edgecolor='black')\n",
    "\n",
    "    plt.xlabel(\"Count\")         # Count on the x-axis\n",
    "    plt.ylabel(col)             # Column values on the y-axis\n",
    "    plt.title(f\"Histogram of {col}\")\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating outliers\n",
    "Outliers in the application order columns we can assume are meaningful "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Preprocessing:\n",
    "Handle missing values (e.g., imputation or removal).\n",
    "Encode categorical features (e.g., one-hot encoding).\n",
    "Normalize or standardize numerical features (especially for Logistic Regression).\n",
    "Split the data into training and testing sets (e.g., 80-20 split).\n",
    "Benchmark Model:\n",
    "Start with Logistic Regression to establish a baseline.\n",
    "Fine-tune hyperparameters (e.g., C, solver, multi_class).\n",
    "Improve with Random Forest:\n",
    "Train and evaluate a Random Forest model.\n",
    "Fine-tune hyperparameters (e.g., n_estimators, max_depth, max_features).\n",
    "Improve Further with XGBoost:\n",
    "Train and evaluate an XGBoost model.\n",
    "Fine-tune hyperparameters (e.g., learning_rate, max_depth, n_estimators).\n",
    "Interpretability:\n",
    "Use SHAP or LIME to explain the predictions of your final model (e.g., XGBoost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What can I check next\n",
    "\n",
    "Before using your multi-class classification data with logistic regression, you should perform several checks to ensure the data is suitable for the model. Logistic regression has specific assumptions and requirements, and your data should meet these for the model to perform well. Here's a checklist:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Check the Target Variable**\n",
    "   - **Encoding**: The target variable should be encoded as integers (e.g., 0, 1, 2, ...) for multi-class logistic regression. Some libraries (like scikit-learn) require this.\n",
    "   - **Class balance**: Check if the classes are balanced. Highly imbalanced classes can lead to poor performance. If imbalanced, consider techniques like oversampling, undersampling, or class weighting.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Check Feature Types**\n",
    "   - **Numerical features**: Logistic regression works best with numerical features. If you have categorical features, encode them (e.g., one-hot encoding, label encoding).\n",
    "   - **Scale of features**: Logistic regression is sensitive to the scale of features. Ensure all features are scaled (e.g., using standardization or normalization).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Check for Multicollinearity**\n",
    "   - Logistic regression assumes that features are not highly correlated with each other. Use techniques like:\n",
    "     - Correlation matrix: Check for high correlations between features.\n",
    "     - Variance Inflation Factor (VIF): Values above 5-10 indicate multicollinearity.\n",
    "   - If multicollinearity exists, consider removing or combining features.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Check for Linearity**\n",
    "   - Logistic regression assumes a linear relationship between features and the log-odds of the target variable. You can check this by:\n",
    "     - Plotting partial dependence plots.\n",
    "     - Using polynomial features if non-linear relationships exist.\n",
    "   - If the relationship is highly non-linear, consider using a different model (e.g., decision trees, SVM).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Check for Outliers**\n",
    "   - Logistic regression can be sensitive to outliers. Check for outliers in your features using:\n",
    "     - Boxplots.\n",
    "     - Z-scores or IQR (Interquartile Range).\n",
    "   - Handle outliers by removing them or transforming the data (e.g., log transformation).\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Check for Missing Values**\n",
    "   - Logistic regression cannot handle missing values directly. Check for missing values in your data and handle them by:\n",
    "     - Imputation (e.g., mean, median, mode).\n",
    "     - Removing rows or columns with missing values (if minimal).\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Check Sample Size**\n",
    "   - Ensure you have enough samples for each class. Logistic regression requires a sufficient number of samples to estimate the coefficients reliably.\n",
    "   - A rule of thumb is to have at least 10 samples per feature per class.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Check for Overfitting**\n",
    "   - Logistic regression can overfit if there are too many features relative to the number of samples. Check the feature-to-sample ratio and consider:\n",
    "     - Feature selection (e.g., using L1 regularization).\n",
    "     - Dimensionality reduction (e.g., PCA).\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **Check for Regularization Needs**\n",
    "   - Logistic regression benefits from regularization (L1 or L2) to prevent overfitting, especially if you have many features. Decide whether to use regularization and tune the regularization parameter.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. **Check for Interpretability**\n",
    "   - Logistic regression is interpretable, so ensure your features are meaningful and interpretable. Avoid using overly complex or engineered features that might reduce interpretability.\n",
    "\n",
    "---\n",
    "\n",
    "### 11. **Check for Software/Library Requirements**\n",
    "   - Ensure your software/library supports multi-class logistic regression. For example:\n",
    "     - In scikit-learn, use `LogisticRegression` with `multi_class='multinomial'` for multi-class problems.\n",
    "     - In statsmodels, use `MNLogit` for multi-class logistic regression.\n",
    "     \n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "Before using logistic regression for multi-class classification, ensure:\n",
    "- The target variable is properly encoded and balanced.\n",
    "- Features are numerical, scaled, and free of multicollinearity.\n",
    "- The data is free of outliers and missing values.\n",
    "- The relationship between features and the target is approximately linear.\n",
    "- You have enough samples and features are interpretable.\n",
    "\n",
    "If your data meets these criteria, it is likely suitable for logistic regression. If not, consider preprocessing or using a different model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit.learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(columns=[\"Target\"])\n",
    "y = df['Target']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for original_class, encoded_value in zip(label_encoder.classes_, range(len(label_encoder.classes_))):\n",
    "    print(f\"Original Class: {original_class} -> Encoded Value: {encoded_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree - benchmark workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "# clf = tree.DecisionTreeClassifier()\n",
    "clf = tree.DecisionTreeClassifier(max_depth=4)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Evaluate on training and testing sets\n",
    "train_accuracy = accuracy_score(y_train, clf.predict(X_train))\n",
    "test_accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Accuracy and Training accuracy show that there is no underfitting or overfitting - model is fitting just right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Go back and improve the decision tree using a grid search, looking at other features and stuff like that\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'n_estimators':[20, 40, 60, 80, 100, 120, 140, 160, 180, 200],\n",
    "#     'max_depth':[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
    "#     'max_features':['sqrt', 'log2', None],\n",
    "#     'min_samples_split':[2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "#     'min_samples_leaf':[1, 2, 3, 4, 5] \n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators':[20, 40, 60, 80, 100, 120, 140, 160, 180, 200],\n",
    "    'max_depth':[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'max_features':['sqrt', 'log2', None],\n",
    "    'min_samples_split':[2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'min_samples_leaf':[1, 2, 3, 4, 5] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "# grid_search = GridSearchCV(rf, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "# grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf, param_grid, n_iter=1000, cv=5, scoring=\"accuracy\", n_jobs=-1, random_state=42, return_train_score=True\n",
    ")\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert cv_results_ to a DataFrame\n",
    "results_df = pd.DataFrame(random_search.cv_results_)\n",
    "\n",
    "# Sort by the mean test score (accuracy) in descending order\n",
    "sorted_results = results_df.sort_values(by='mean_test_score', ascending=False)\n",
    "\n",
    "# Get the top 3 or 5 parameter combinations\n",
    "top_n = 3  # Change this to 5 if you want the top 5\n",
    "top_combinations = sorted_results.head(top_n)\n",
    "\n",
    "# Display the top parameter combinations and their scores\n",
    "print(f\"Top {top_n} Parameter Combinations:\")\n",
    "print(top_combinations[['params', 'mean_test_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = sorted_results[sorted_results[\"mean_train_score\"]>=0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df['param_min_samples_leaf'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the absolute difference between mean_train_score and mean_test_score\n",
    "temp_df['score_diff'] = abs(temp_df['mean_train_score'] - temp_df['mean_test_score'])\n",
    "\n",
    "# Step 2: Filter rows where the difference is at most 6% (0.06)\n",
    "filtered_df = temp_df[temp_df['score_diff'] <= 0.05]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "# Method 1: Using os.cpu_count()\n",
    "print(f\"Number of CPU cores (os.cpu_count()): {os.cpu_count()}\")\n",
    "\n",
    "# Method 2: Using multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores (multiprocessing.cpu_count()): {multiprocessing.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[filtered_df[\"mean_train_score\"]>=0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ADD CLASSIFICATION REPORT, CONFUSION MATRIX, ACCURACY, RECALL, PRECISION, F1 SCORE - IN THE DECISION TREE STUFF TOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['param_max_depth'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best Parameters for Random Forest: {random_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {random_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42, n_estimators=180, min_samples_split=9, min_samples_leaf=3, max_features='sqrt', max_depth=6)\n",
    "rf.fit(X_train, y_train)\n",
    "train_accuracy = accuracy_score(y_train, rf.predict(X_train))\n",
    "test_accuracy = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Best Parameters for Random Forest: {grid_search.best_params_}\")\n",
    "# print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Evaluate on training and testing sets\n",
    "train_accuracy = accuracy_score(y_train, rf.predict(X_train))\n",
    "test_accuracy = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "# Train the model\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=18,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn==1.5.2\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Initialize XGBClassifier\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'gamma': uniform(0, 0.5),\n",
    "    'min_child_weight': randint(1, 6),\n",
    "    'reg_alpha': uniform(0, 1),\n",
    "    'reg_lambda': uniform(0, 1),\n",
    "    'objective': ['multi:softmax'],\n",
    "    'num_class': [3],\n",
    "    'eval_metric': ['mlogloss']\n",
    "}\n",
    "\n",
    "# Run RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb, param_grid, n_iter=100, cv=5, scoring=\"accuracy\", n_jobs=-1, random_state=42\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "print(\"Best Parameters:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(colsample_bytree=0.8940284175215543, eval_metric='mlogloss', gamma=0.4017404651924243, learning_rate=0.09461037177139194, max_depth=4, min_child_weight=5, n_estimators=148, num_class=3, objective='multi:softmax', reg_alpha=0.4126176769114265, reg_lambda=0.37201808579278317, subsample=0.9105651842967988,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Evaluate on training and testing sets\n",
    "xgb = xgb.fit(X_train, y_train)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, xgb.predict(X_train))\n",
    "test_accuracy = accuracy_score(y_test, xgb.predict(X_test))\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import xgboost\n",
    "\n",
    "print(\"scikit-learn version:\", sklearn.__version__)\n",
    "print(\"XGBoost version:\", xgboost.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap\n",
    "import shap\n",
    "\n",
    "# Train an XGBoost model\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Explain the model's predictions using SHAP\n",
    "explainer = shap.TreeExplainer(xgb)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Plot SHAP summary\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base models\n",
    "base_models = [\n",
    "    ('random_forest', RandomForestClassifier(random_state=42,max_depth=6)),\n",
    "    ('decision_tree', tree.DecisionTreeClassifier(random_state=42,max_depth=4)),\n",
    "    ('xgboost', XGBClassifier(random_state=42))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define meta-model (Decision Tree)\n",
    "meta_model = tree.DecisionTreeClassifier(random_state=42,max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "# Create stacking classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,  # Use 5-fold cross-validation to generate base model predictions\n",
    "    stack_method='auto',  # Use 'predict_proba' if possible, otherwise 'predict'\n",
    "    n_jobs=-1  # Use all available CPU cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the stacking model\n",
    "stacking_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Stacking Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "principles__of_ai_and_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
